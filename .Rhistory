ap = c(.99, .95, .95, .01)
r = c(.9, .9, .2, .2)
probs <- cbind(wt, e, sp, ap, r)
xtable(probs, digits=3)
r1=prod(wt[1], e[1], sp[1], ap[1], r[1])
r2=prod(wt[2], e[2], sp[2], ap[2], r[1])
r3=prod(wt[3], e[3], sp[3], ap[3], r[3])
r4=prod(wt[4], e[4], sp[4], ap[4], r[4])
sum(r1, r2)/sum(r1, r2, r3, r4)
sum(r3, r4)/sum(r1, r2, r3, r4)
######## Part 1 ###########
library(xtable)
wt = c(.95, .95, .95, .95)
e = c(.001,.001,.999,.999)
sp = c(.1, .9, .1, .9)
ap = c(.99, .95, .95, .01)
r = c(.9, .9, .2, .2)
probs <- cbind(wt, e, sp, ap, r)
xtable(probs, digits=3)
r1=prod(wt[1], e[1], sp[1], ap[1], r[1])
r2=prod(wt[2], e[2], sp[2], ap[2], r[1])
r3=prod(wt[3], e[3], sp[3], ap[3], r[3])
r4=prod(wt[4], e[4], sp[4], ap[4], r[4])
sum(r1, r2)/sum(r1, r2, r3, r4)
sum(r3, r4)/sum(r1, r2, r3, r4)
probs
wt = c(.02, .02, .02, .02)
e = c(.04,.04,.96,.96)
sp = c(.2, .8, .2, .8)
ap = c(.99, .95, .95, .01)
r = c(.9, .9, .2, .2)
probs_week2 <- cbind(wt, e, sp, ap, r)
xtable(probs_week2, digits=3)
r1=prod(wt[1], e[1], sp[1], ap[1], r[1])
r2=prod(wt[2], e[2], sp[2], ap[2], r[1])
r3=prod(wt[3], e[3], sp[3], ap[3], r[3])
r4=prod(wt[4], e[4], sp[4], ap[4], r[4])
sum(r1, r2)/sum(r1, r2, r3, r4)
sum(r3, r4)/sum(r1, r2, r3, r4)
######## Part 3 ###########
weeks_t <- c(.95, .95, .95, .95, .03, .03, .03, .03, .02, .02, .02, .02)
e_w <- c(.001, .001, .999, .999, .02, .02, .98, .98, .04, .04, .96, .96)
r_e <- c(.9, .1, .2, .8, .9, .1, .2, .8, .9, .1, .2, .8)
probs2 <- cbind(weeks_t, e_w, r_e)
row.names(probs2) <- NULL
R <- apply(probs2, 1, prod)
condensed <- c(sum(R[1], R[3]), sum(R[2], R[4]), sum(R[5], R[7]),
sum(R[6], R[8]),sum(R[9], R[11]), sum(R[10], R[12]))
dist_true <- condensed[c(1, 3, 5)]/sum(condensed[c(1, 3, 5)])
weeks = c(0, 1, 2)
res <- rbind(weeks, dist_true)
xtable(res, digits=2)
dist_false <- condensed[c(2, 4, 6)]/sum(condensed[c(2, 4, 6)])
exp_false <- 1*dist_false[2] + 2*dist_false[3]
dist_true
exp_false
#########################################################################
## Purpose: Using probabilities for each game result, simulate the season
## to compute a distribution of wins ####################################
#########################################################################
## SET WORKING DIRCTORY ##
setwd("C:/Users/Lee/game_simulation")
## LIBRARIES
library("dplyr")
library("e1071")
## READ IN OUR FEATURE DATASETS
data <- read.csv("scripts/rpm_dataset.csv")
## ADD home feature and win/loss column
data <- mutate(data, home = 1)
data$homeWin <- ifelse(data$home_team_score > data$visit_team_score, 1, 0)
## Set up datasets ##
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008, 2009, 2010, 2011, 2012))
test = filter(data, game_year == 2013)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
## Naive Bayes to get probabilities
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
## Save dataset with the correct probabilities
probs <- cbind(test, preds)[, c(2:8, 17:22)]
## Rename home and away columns
names(probs)[names(probs) == "0"] <- "away_prob"
names(probs)[names(probs) == "1"] <- "home_prob"
##Create the dataset for simulation
num_seasons <- 1000
season_df <- data.frame(matrix(0, nrow=length(unique(probs$home_team)), ncol= num_seasons))
row.names(season_df) <- unique(probs$home_team)
colnames(season_df) = paste("season_", 1:1000, sep="")
## Get the unique match Id's for a given season
match_ids <- unique(probs$match_id)
## Get random number for each_row
random_outcomes <- runif(length(probs[,1]))
probs <- cbind(probs, random_outcomes)
## SET WORKING DIRCTORY ##
setwd("C:/Users/Lee/game_simulation")
## LIBRARIES
library("dplyr")
library("e1071")
## READ IN OUR FEATURE DATASETS
data <- read.csv("scripts/rpm_dataset.csv")
head(data)
table(data$game_year)
data <- read.csv("scripts/rpm_dataset.csv")
## ADD home feature and win/loss column
data <- mutate(data, home = 1)
data$homeWin <- ifelse(data$home_team_score > data$visit_team_score, 1, 0)
## Set up datasets ##
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data, game_year == 2012)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
## Logistic Regression
mylogit <- glm(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home, data=train,
family = "binomial")
logit_preds <- as.data.frame(predict(mylogit, newdata=xtest, type="response"))
logit_preds$class <- ifelse(logit_preds[,1] >= .5, 1, 0)
logit_preds <- cbind(logit_preds, ytest)
logit_preds$result <- abs(logit_preds[,2] - logit_preds[,3])
logit_accurary <- 1 - sum(logit_preds$result)/length(ytest)
logit_accurary
install.packages("randomForest")
library("randomForest")
fit <- randomForest(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home, data=train)
print(fit) # view results
importance(fit) # importance of each predictor
print(git)
print(fit)
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2011))
test = filter(data, game_year == 2012)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data, game_year == 2012)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
table(data$home)
head(data)
?summarise
summarise(data, group_by(game_year), mean(homeWin))
library("dplyr")
library("e1071")
library("randomForest")
summarise(data, group_by(game_year), mean(homeWin))
data_rpi <- read.csv("rpi.csv")
data_rpi <- read.csv("rpi")
## ADD home feature and win/loss column
getwd()
data_rpi <- read.csv("scripts/rpi")
data_rpi <- read.csv("scripts/rpi.csv")
head(data_rpi)
?merge
data_combined <- merge(data, data_rpi, all.x=TRUE)
head(data_combined)
log(data_combined$scores.away_rpi)
data <- read.csv("scripts/rpm_dataset.csv")
data_rpi <- read.csv("scripts/rpi.csv")
data_combined <- merge(data, data_rpi, all.x=TRUE)
## ADD home feature and win/loss column
data_combined <- mutate(data_combined, home = 1)
data_combined$homeWin <- ifelse(data_combined$home_team_score > data_combined$visit_team_score, 1, 0)
head(data_combined)
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data_combined, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data_combined, game_year == 2012)
head(xtest)
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data_combined, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data_combined, game_year == 2012)
names(test)
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data_combined, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data_combined, game_year == 2012)
xtest = test[,9:22]
ytest = test[,23]
xtrain = train[,9:22]
ytrain = train[,23]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
names(train)
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data_combined, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data_combined, game_year == 2012)
xtest = test[,c(20:22)]
ytest = test[,23]
xtrain = train[,c(20:22)]
ytrain = train[,23]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
names(xtrain)
names(train)
xtest = test[,c(18, 19, 22)]
ytest = test[,23]
xtrain = train[,c(18, 19, 22)]
ytrain = train[,23]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data_combined, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data_combined, game_year == 2012)
xtest = test[,c(18, 19, 22)]
ytest = test[,23]
xtrain = train[,c(18, 19, 22)]
ytrain = train[,23]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
data <- read.csv("scripts/rpm_dataset.csv")
data_rpi <- read.csv("scripts/rpi.csv")
data_combined <- merge(data, data_rpi, all.x=TRUE)
## ADD home feature and win/loss column
data <- mutate(data, home = 1)
data$homeWin <- ifelse(data$home_team_score > data$visit_team_score, 1, 0)
## Set up datasets ##
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data, game_year == 2012)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
## Logistic Regression
mylogit <- glm(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home, data=train,
family = "binomial")
logit_preds <- as.data.frame(predict(mylogit, newdata=xtest, type="response"))
logit_preds$class <- ifelse(logit_preds[,1] >= .5, 1, 0)
logit_preds <- cbind(logit_preds, ytest)
logit_preds$result <- abs(logit_preds[,2] - logit_preds[,3])
logit_accurary <- 1 - sum(logit_preds$result)/length(ytest)
logit_accurary
## Linear Regression
mylinear <- lm(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home, data=train)
linear_preds <- as.data.frame(predict(mylinear, newdata=xtest, type="response"))
linear_preds$class <- ifelse(linear_preds[,1] >= .5, 1, 0)
linear_preds <- cbind(linear_preds, ytest)
linear_preds$result <- abs(linear_preds[,2] - linear_preds[,3])
linear_accurary <- 1 - sum(linear_preds$result)/length(ytest)
linear_accurary
## Random Forest
fit <- randomForest(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home, data=train)
print(fit) # view results
importance(fit) # importance of each predictor
detach(package:plyr)
detach(package:plyr)
?detach
detach(package:plyr)
summarise(data, group_by(game_year), mean(homeWin))
head(data)
setwd("C:/Users/Lee/game_simulation")
## LIBRARIES
library("dplyr")
library("e1071")
library("randomForest")
## READ IN OUR FEATURE DATASETS
data <- read.csv("scripts/rpm_dataset.csv")
data_rpi <- read.csv("scripts/rpi.csv")
## ADD home feature and win/loss column
data <- mutate(data, home = 1)
data$homeWin <- ifelse(data$home_team_score > data$visit_team_score, 1, 0)
## Set up datasets ##
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data, game_year == 2012)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
group_by(data, game_year)
grouped <- group_by(data, game_year)
summarise(grouped, mean(homeWin))
home_summary <- summarise(grouped, mean(homeWin))
print(fit)
predict(fit, test)
length(predict(fit, test))
table(data$game_year)
rf <- randomForest(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home, data=train)
?randomForest
rf <- randomForest(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home,
data=train, type="classification")
rf_preds <- predict(rf, data=test)
head(rf_preds)
rf_preds <- cbind(rf_preds, ytest)
head(rf_preds)
rf_preds$class <- ifelse(rf_preds[,1] >= .5, 1, 0)
head(rf_preds)
rf_preds <- as.data.frame(predict(rf, data=test))
rf_preds <- cbind(rf_preds, ytest)
length(ytest)
length(rf_preds)
head(rf_preds)
length(rf_preds)
rf_preds <- as.data.frame(predict(rf, data=test))
rf_preds <- as.numeric(predict(rf, data=test))
rf_preds <- as.numeric(predict(rf, data=xtest))
rf_preds
data <- mutate(data, home = 1)
data$homeWin <- ifelse(data$home_team_score > data$visit_team_score, 1, 0)
## Set up datasets ##
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data, game_year == 2012)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
dim(xtest)
rf <- randomForest(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home,
data=train, xtest = xtest, ytest=ytest, type="classification")
rf_preds <- as.numeric(predict(rf, data=xtest))
print(rf)
rf <- randomForest(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home,
data=train, type="classification")
rf_preds <- predict(rf, xtest)
typeof(rf_preds)
rf_preds <- as.data.frame(predict(rf, xtest))
rf_preds <- cbind(rf_preds, ytest)
head(rf_preds)
rf_preds$class <- ifelse(rf_preds[,1] >= .5, 1, 0)
rf_preds$class <- ifelse(rf_preds[,1] >= .5, 1, 0)
rf_preds$result <- abs(rf_preds[,2] - rf_preds[,3])
head(rf_preds)
rf_accurary <- 1 - sum(rf_preds$result)/length(ytest)
rf_accurary
## Set up datasets ##
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008, 2009, 2010, 2011, 2012))
test = filter(data, game_year == 2013)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
## Logistic Regression
mylogit <- glm(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home, data=train,
family = "binomial")
logit_preds <- as.data.frame(predict(mylogit, newdata=xtest, type="response"))
logit_preds$class <- ifelse(logit_preds[,1] >= .5, 1, 0)
logit_preds <- cbind(logit_preds, ytest)
logit_preds$result <- abs(logit_preds[,2] - logit_preds[,3])
logit_accurary <- 1 - sum(logit_preds$result)/length(ytest)
logit_accurary
## Linear Regression
mylinear <- lm(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home, data=train)
linear_preds <- as.data.frame(predict(mylinear, newdata=xtest, type="response"))
linear_preds$class <- ifelse(linear_preds[,1] >= .5, 1, 0)
linear_preds <- cbind(linear_preds, ytest)
linear_preds$result <- abs(linear_preds[,2] - linear_preds[,3])
linear_accurary <- 1 - sum(linear_preds$result)/length(ytest)
linear_accurary
## Random Forest
rf <- randomForest(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home,
data=train, type="classification")
rf_preds <- as.data.frame(predict(rf, xtest))
rf_preds <- cbind(rf_preds, ytest)
rf_preds$class <- ifelse(rf_preds[,1] >= .5, 1, 0)
rf_preds$result <- abs(rf_preds[,2] - rf_preds[,3])
rf_accurary <- 1 - sum(rf_preds$result)/length(ytest)
rf_accurary
train = filter(data, game_year %in% c(2008, 2009, 2010))
test = filter(data, game_year == 2011)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
rf <- randomForest(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home,
data=train, type="classification")
rf_preds <- as.data.frame(predict(rf, xtest))
rf_preds <- cbind(rf_preds, ytest)
rf_preds$class <- ifelse(rf_preds[,1] >= .5, 1, 0)
rf_preds$result <- abs(rf_preds[,2] - rf_preds[,3])
rf_accurary <- 1 - sum(rf_preds$result)/length(ytest)
rf_accurary
train = filter(data, game_year %in% c(2008, 2009))
test = filter(data, game_year == 2010)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
rf <- randomForest(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home,
data=train, type="classification")
rf_preds <- as.data.frame(predict(rf, xtest))
rf_preds <- cbind(rf_preds, ytest)
rf_preds$class <- ifelse(rf_preds[,1] >= .5, 1, 0)
rf_preds$result <- abs(rf_preds[,2] - rf_preds[,3])
rf_accurary <- 1 - sum(rf_preds$result)/length(ytest)
rf_accurary
train = filter(data, game_year %in% c(2008))
test = filter(data, game_year == 2009)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds$result <- abs(preds[,3] - preds[,4])
preds <- cbind(preds, ytest)
accuracy <- 1 - sum(preds$result)/length(ytest)
## Naive Bayes
accuracy
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
model <- naiveBayes(xtrain, ytrain)
rf <- randomForest(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home,
data=train, type="classification")
rf_preds <- as.data.frame(predict(rf, xtest))
rf_preds <- cbind(rf_preds, ytest)
rf_preds$class <- ifelse(rf_preds[,1] >= .5, 1, 0)
rf_preds$result <- abs(rf_preds[,2] - rf_preds[,3])
rf_accurary <- 1 - sum(rf_preds$result)/length(ytest)
rf_accurary
