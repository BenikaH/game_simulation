abline(v=results_2013[results_2013$team == name, "wins"], col="red")
abline(v=results_2013[results_2013$team == name, "lower"], col="blue")
abline(v=results_2013[results_2013$team == name, "upper"], col="blue")
}
results <- read.csv("data/espn_data/team_wins.csv")[,2:4]
results_2013 <- filter(results, year == 2013)[order(results_2013$team),]
## Add in the confidence interval
results_2013 <- cbind(results_2013, means[order(names(means))])
results_2013 <- cbind(results_2013, ses[order(names(ses))])
names(results_2013)[4] = "estimate"
names(results_2013)[5] = "se"
results_2013$lower <- results_2013$estimate - 2*results_2013$se
results_2013$upper <- results_2013$estimate + 2*results_2013$se
results_2013$trapped <- ifelse(results_2013$wins >= results_2013$lower &
results_2013$wins <= results_2013$upper, 1, 0)
## Plot the outcomes
par(mfrow=c(6, 5))
team_names <- unique(results_2013$team)
for(name in team_names){
season_result <- as.numeric(season_df[name,])
hist(season_result, main=paste("Wins for", toupper(name), sep=" "), xlab="Wins"
, breaks=10)
abline(v=results_2013[results_2013$team == name, "wins"], col="red")
abline(v=results_2013[results_2013$team == name, "lower"], col="blue")
abline(v=results_2013[results_2013$team == name, "upper"], col="blue")
}
## Get 2013 Results
results_2013 <- filter(results, year == 2013)
results_2013_sorted <- results_2013[order(results_2013$team),]
## Get the dataset with simulation output for 2013
simulation <- read.csv("scripts/sim_2013_logit.csv")
sim_sort_logit <- simulation[order(simulation$X),]
## Get comparison dataframe and mean and absolute error losses
compare_2013_logit <- cbind(results_2013_sorted, sim_sort_logit)
compare_2013_logit$squared <- (compare_2013_logit$means - compare_2013_logit$wins)^2
rmse <- sqrt(mean(compare_2013_logit$squared))
rmse
compare_2013_logit$absolute <- abs(compare_2013_logit$means - compare_2013_logit$wins)
mae <- mean(compare_2013_logit$absolute)
mae
## Purpose: Figure out most important covariates for prediction
library(dplyr)
## SET WORKING DIRCTORY ##
setwd("C:/Users/leeri_000/basketball_stats/game_simulation")
full_matrix <- read.csv("featuresAll.csv")
dim(full_matrix)
head(full_matrix)
full_matrix <- mutate(full_matrix, home = 1)
?step
names(full_matrix)
full_mod <- glm(homeWin ~ full_matrix[,8:45], family="binomial", data=full_matrix)
full_mod <- glm(homeWin ~., family="binomial", data=full_matrix)
summary(glm)
full_matrix <- as.factor(mutate(full_matrix, home = 1))
full_matrix$home <- factor(full_matrix$home)
head(full_matrix)
summary(glm(homeWin ~ 1, data=full_matrix, family="binomial"))
null_mod <- glm(formula = homeWin ~ 1, family = "binomial", data = full_matrix)
summary(null_mod)
names(full_matrix)
null_mod <- glm(formula = homeWin ~ 1, family = "binomial", data = full_matrix)
full_mod <- glm(homeWin ~ RPM_weight_0 + ORPM_weight_0 + DRPM_weight_0 + PER_weight_0 +
RPM_weight_1 + ORPM_weight_1 + DRPM_weight_1 + PER_weight_1
,family="binomial", data=full_matrix)
summary(full_mod(
summary(full_mod)
summary(full_mod)
names(full_matrix)
full_mod <- glm(homeWin ~ RPM_weight_0 + ORPM_weight_0 + DRPM_weight_0 + PER_weight_0 +
RPM_weight_1 + ORPM_weight_1 + DRPM_weight_1 + PER_weight_1 + home + avg_scoreDiff +
avg_homeWin + avg_scoreDiff_home+avg_win_home+avg_scoreDiff_visit+avg_win_visit+home_rpi+away_rpi+avg_GP+avg_GS+avg_MIN+avg_FG_made+avg_FG_attempted+
avg_FGpercent+ avg_ThreeP_made+avg_ThreeP_attempted+avg_ThreePpercent+avg_FT_made+
avg_FT_attempted+avg_FTpercent+ avg_OR+avg_DR+avg_REB+avg_AST+avg_BLK+
avg_STL+avg_PF+avg_TO+avg_PTS,family="binomial", data=full_matrix)
full_mod <- glm(homeWin ~ RPM_weight_0 + ORPM_weight_0 + DRPM_weight_0 + PER_weight_0 +
RPM_weight_1 + ORPM_weight_1 + DRPM_weight_1 + PER_weight_1 + home + avg_scoreDiff +
avg_homeWin + avg_scoreDiff_home+avg_win_home+avg_scoreDiff_visit+avg_win_visit+home_rpi+
away_rpi+avg_GP+avg_GS+avg_MIN+avg_FG_made+avg_FG_attempted+
avg_FGpercent+ avg_ThreeP_made+avg_ThreeP_attempted+avg_ThreePpercent+avg_FT_made+
avg_FT_attempted+avg_FTpercent+avg_OR+avg_DR+avg_REB+avg_AST+avg_BLK+
avg_STL+avg_PF+avg_TO+avg_PTS,family="binomial", data=full_matrix)
head(full_matrix
)
full_mod <- glm(homeWin ~ RPM_weight_0 + ORPM_weight_0 + DRPM_weight_0 + PER_weight_0 +
RPM_weight_1 + ORPM_weight_1 + DRPM_weight_1 + PER_weight_1 + home + avg_scoreDiff +
avg_scoreDiff_home+avg_win_home+avg_scoreDiff_visit+avg_win_visit+home_rpi+
away_rpi+avg_GP+avg_GS+avg_MIN+avg_FG_made+avg_FG_attempted+
avg_FGpercent+ avg_ThreeP_made+avg_ThreeP_attempted+avg_ThreePpercent+avg_FT_made+
avg_FT_attempted+avg_FTpercent+avg_OR+avg_DR+avg_REB+avg_AST+avg_BLK+
avg_STL+avg_PF+avg_TO+avg_PTS,family="binomial", data=full_matrix)
full_mod <- glm(homeWin ~ RPM_weight_0 + ORPM_weight_0 + DRPM_weight_0 + PER_weight_0 +
RPM_weight_1 + ORPM_weight_1 + DRPM_weight_1 + PER_weight_1 + home,
family="binomial", data=full_matrix)
full_mod <- glm(homeWin ~ RPM_weight_0 + ORPM_weight_0 + DRPM_weight_0 + PER_weight_0 +
RPM_weight_1 + ORPM_weight_1 + DRPM_weight_1 + PER_weight_1 + avg_scoreDiff +
avg_scoreDiff_home + avg_win_home + avg_scoreDiff_visit + avg_win_visit + home_rpi +
away_rpi + avg_GP + avg_GS+avg_MIN + avg_FG_made + avg_FG_attempted +
avg_FGpercent + avg_ThreeP_made + avg_ThreeP_attempted + avg_ThreePpercent + avg_FT_made +
avg_FT_attempted+ avg_FTpercent+ avg_OR+ avg_DR+ avg_REB+ avg_AST + avg_BLK+
avg_STL+ avg_PF +avg_TO+ avg_PTS, family="binomial", data=full_matrix)
step(null_mod, scope=list(lower=null_mod, upper=full_mod), direction=both)
step(null_mod, scope=list(lower=null_mod, upper=full_mod), direction="both")
## SET WORKING DIRCTORY ##
setwd("C:/Users/Lee/game_simulation")
## LIBRARIES
library("dplyr")
library("e1071")
library("randomForest")
## READ IN OUR FEATURE DATASETS
data <- read.csv("scripts/rpm_dataset.csv")
data_rpi <- read.csv("scripts/rpi.csv")
## ADD home feature and win/loss column
data <- mutate(data, home = 1)
data$homeWin <- ifelse(data$home_team_score > data$visit_team_score, 1, 0)
## Set up datasets ##
train = filter(data, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data, game_year == 2012)
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008))
test = filter(data, game_year == 2009)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
mylogit <- glm(homeWin ~ RPM_weight.0 + RPM_weight.1, data=train, family = "binomial")
summary(mylogit)
mylogit <- glm(homeWin ~ RPM_weight.0 + RPM_weight.1, data=train, family = "binomial")
logit_preds <- as.data.frame(predict(mylogit, newdata=xtest, type="response"))
logit_preds$class <- ifelse(logit_preds[,1] >= .5, 1, 0)
logit_preds <- cbind(logit_preds, ytest)
logit_preds$result <- abs(logit_preds[,2] - logit_preds[,3])
logit_accurary <- 1 - sum(logit_preds$result)/length(ytest)
logit_accurary
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008, 2009))
test = filter(data, game_year == 2010)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
## Logistic Regression
mylogit <- glm(homeWin ~ RPM_weight.0 + RPM_weight.1, data=train, family = "binomial")
logit_preds <- as.data.frame(predict(mylogit, newdata=xtest, type="response"))
logit_preds$class <- ifelse(logit_preds[,1] >= .5, 1, 0)
logit_preds <- cbind(logit_preds, ytest)
logit_preds$result <- abs(logit_preds[,2] - logit_preds[,3])
logit_accurary <- 1 - sum(logit_preds$result)/length(ytest)
logit_accurary
train = filter(data, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data, game_year == 2012)
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008, 2009, 2011))
test = filter(data, game_year == 2012)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
## Logistic Regression
mylogit <- glm(homeWin ~ RPM_weight.0 + RPM_weight.1, data=train, family = "binomial")
logit_preds <- as.data.frame(predict(mylogit, newdata=xtest, type="response"))
logit_preds$class <- ifelse(logit_preds[,1] >= .5, 1, 0)
logit_preds <- cbind(logit_preds, ytest)
logit_preds$result <- abs(logit_preds[,2] - logit_preds[,3])
logit_accurary <- 1 - sum(logit_preds$result)/length(ytest)
logit_accurary
## Set up datasets ##
train = filter(data, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data, game_year == 2012)
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008, 2009, 2011, 2012))
test = filter(data, game_year == 2013)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
## Logistic Regression
mylogit <- glm(homeWin ~ RPM_weight.0 + RPM_weight.1, data=train, family = "binomial")
logit_preds <- as.data.frame(predict(mylogit, newdata=xtest, type="response"))
logit_preds$class <- ifelse(logit_preds[,1] >= .5, 1, 0)
logit_preds <- cbind(logit_preds, ytest)
logit_preds$result <- abs(logit_preds[,2] - logit_preds[,3])
logit_accurary <- 1 - sum(logit_preds$result)/length(ytest)
logit_accurary
mylogit <- glm(homeWin ~ RPM_weight.0 + RPM_weight.1 + home, data=train, family = "binomial")
logit_preds <- as.data.frame(predict(mylogit, newdata=xtest, type="response"))
logit_preds$class <- ifelse(logit_preds[,1] >= .5, 1, 0)
logit_preds <- cbind(logit_preds, ytest)
logit_preds$result <- abs(logit_preds[,2] - logit_preds[,3])
logit_accurary <- 1 - sum(logit_preds$result)/length(ytest)
logit_accurary
data <- mutate(data, home = 1)
data$homeWin <- ifelse(data$home_team_score > data$visit_team_score, 1, 0)
## Set up datasets ##
train = filter(data, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data, game_year == 2012)
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008, 2009, 2011, 2012))
test = filter(data, game_year == 2013)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
## Logistic Regression
mylogit <- glm(homeWin ~ RPM_weight.0 + RPM_weight.1, data=train, family = "binomial")
logit_preds <- as.data.frame(predict(mylogit, newdata=xtest, type="response"))
logit_preds$class <- ifelse(logit_preds[,1] >= .5, 1, 0)
logit_preds <- cbind(logit_preds, ytest)
logit_preds$result <- abs(logit_preds[,2] - logit_preds[,3])
logit_accurary <- 1 - sum(logit_preds$result)/length(ytest)
logit_accurary
## Linear Regression
mylinear <- lm(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home, data=train)
linear_preds <- as.data.frame(predict(mylinear, newdata=xtest, type="response"))
linear_preds$class <- ifelse(linear_preds[,1] >= .5, 1, 0)
linear_preds <- cbind(linear_preds, ytest)
linear_preds$result <- abs(linear_preds[,2] - linear_preds[,3])
linear_accurary <- 1 - sum(linear_preds$result)/length(ytest)
linear_accurary
## Random Forest
rf <- randomForest(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home,
data=train, type="classification")
rf_preds <- as.data.frame(predict(rf, xtest))
rf_preds <- cbind(rf_preds, ytest)
rf_preds$class <- ifelse(rf_preds[,1] >= .5, 1, 0)
rf_preds$result <- abs(rf_preds[,2] - rf_preds[,3])
rf_accurary <- 1 - sum(rf_preds$result)/length(ytest)
rf_accurary
install_packages("randomForest")
install.packages("randomForest")
setwd("C:/Users/Lee/game_simulation")
## LIBRARIES
library("dplyr")
library("e1071")
library("randomForest")
## READ IN OUR FEATURE DATASETS
data <- read.csv("scripts/rpm_dataset.csv")
data_rpi <- read.csv("scripts/rpi.csv")
## ADD home feature and win/loss column
data <- mutate(data, home = 1)
data$homeWin <- ifelse(data$home_team_score > data$visit_team_score, 1, 0)
## Set up datasets ##
train = filter(data, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(data, game_year == 2012)
years <- c(2008, 2009, 2010, 2011, 2012, 2013)
train = filter(data, game_year %in% c(2008, 2009, 2011, 2012))
test = filter(data, game_year == 2013)
xtest = test[,9:17]
ytest = test[,18]
xtrain = train[,9:17]
ytrain = train[,18]
## Naive Bayes
model <- naiveBayes(xtrain, ytrain)
preds <- as.data.frame(predict(model, xtest, type = c("raw"), threshold = 0.001))
preds$class <- ifelse(preds[,2] > preds[,1], 1, 0)
preds <- cbind(preds, ytest)
preds$result <- abs(preds[,3] - preds[,4])
accuracy <- 1 - sum(preds$result)/length(ytest)
accuracy
## Logistic Regression
mylogit <- glm(homeWin ~ RPM_weight.0 + RPM_weight.1, data=train, family = "binomial")
logit_preds <- as.data.frame(predict(mylogit, newdata=xtest, type="response"))
logit_preds$class <- ifelse(logit_preds[,1] >= .5, 1, 0)
logit_preds <- cbind(logit_preds, ytest)
logit_preds$result <- abs(logit_preds[,2] - logit_preds[,3])
logit_accurary <- 1 - sum(logit_preds$result)/length(ytest)
logit_accurary
## Linear Regression
mylinear <- lm(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home, data=train)
linear_preds <- as.data.frame(predict(mylinear, newdata=xtest, type="response"))
linear_preds$class <- ifelse(linear_preds[,1] >= .5, 1, 0)
linear_preds <- cbind(linear_preds, ytest)
linear_preds$result <- abs(linear_preds[,2] - linear_preds[,3])
linear_accurary <- 1 - sum(linear_preds$result)/length(ytest)
linear_accurary
## Random Forest
rf <- randomForest(homeWin ~ RPM_weight.0 + ORPM_weight.0 + DRPM_weight.0 + PER_weight.0 +
RPM_weight.1 + ORPM_weight.1 + DRPM_weight.1 + PER_weight.1 + home,
data=train, type="classification")
rf_preds <- as.data.frame(predict(rf, xtest))
rf_preds <- cbind(rf_preds, ytest)
rf_preds$class <- ifelse(rf_preds[,1] >= .5, 1, 0)
rf_preds$result <- abs(rf_preds[,2] - rf_preds[,3])
rf_accurary <- 1 - sum(rf_preds$result)/length(ytest)
rf_accurary
mylogit <- glm(homeWin ~ RPM_weight.0 + RPM_weight.1 + PER_weight.0 + PER_weight.1, data=train, family = "binomial")
summary(mylogit
)
mylogit <- glm(homeWin ~ RPM_weight.0 + RPM_weight.1 + PER_weight.0 + PER_weight.1, data=train, family = "binomial")
logit_preds <- as.data.frame(predict(mylogit, newdata=xtest, type="response"))
logit_preds$class <- ifelse(logit_preds[,1] >= .5, 1, 0)
logit_preds <- cbind(logit_preds, ytest)
logit_preds$result <- abs(logit_preds[,2] - logit_preds[,3])
logit_accurary <- 1 - sum(logit_preds$result)/length(ytest)
logit_accurary
train = filter(full_matrix, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(full_matrix, game_year == 2012)
head(train)
library(lars)
install.packages("lars")
library(lars)
## SET WORKING DIRCTORY ##
setwd("C:/Users/leeri_000/basketball_stats/game_simulation")
# Read in the full feature matrix
full_matrix <- read.csv("featuresAll.csv")
full_matrix <- read.csv("featuresAll.csv")
head(full_matrix)
names(full_matrix)
train = filter(full_matrix, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(full_matrix, game_year == 2012)
xtest = test[,8:44]
ytest = test[,2]
xtrain = train[,8:44]
ytrain = train[,2]
head(train)
dim(train)
dim(test)
?lars
lars(xtrain, ytrain, type="lasso")
lars(xtrain, ytrain)
xtest = as.matrix(test[,8:44])
ytest = as.matrix(test[,2])
xtrain = as.matrix(train[,8:44])
ytrain = as.matrix(train[,2])
# Fit a LARS object
lars(xtrain, ytrain)
source('~/.active-rstudio-document', echo=TRUE)
lasso_mod <- lars(xtrain, ytrain, type="lasso")
names(lasso_mod)
lasso_mod$beta
summary(lasso_mod)
fits <- predict.lars(lasso_mod, xtest, type="fit")
head(fits)
dim(fits
)
str(fits)
coef4.1 <- coef(lasso_mod, s=4.1, mode="norm")
coef4.1
coef4.1 <- predict(lasso_mod, s=4.1, type="coef", mode="norm")
coef4.1
fits <- predict.lars(lasso_mod, xtest, type="fit")
head(fits)
lasso_mod
names(lasso_mod)
lasso_mod$coef
lasso_mod$lambda
lasso_mod$type
lasso_mod$call
lasso_mod$entry
lasso_mod$beta
lasso_mod$mu
fits <- predict.lars(lasso_mod, xtest, type="fit")
fits <- predict.lars(lasso_mod, xtest, type="fit")
fits[1]
fits[2]
fits[3]
fits[4]
dim(fits[4])
length(fits[4])
head(fits)
str(fits)
fits[,56]
fits[4,56]
fits[4]
fits[4,55]
p <- fits[4]
p <- as.data.frame(fits[4])
head(p)
p[,56]
length(p[,56])
lasso_probs <- p[,56]
lasso_probs$class <- ifelse(lasso_probs[,2] > lasso_probs[,1], 1, 0)
lasso_probs$class <- ifelse(lasso_probs[,1] >= .5, 1, 0)
lasso_probs <- as.data.frame(p[,56])
lasso_probs$class <- ifelse(lasso_probs[,1] >= .5, 1, 0)
lasso_probs
lasso_probs <- cbind(lasso_probs, ytest)
lasso_probs
lasso_probs$result <- abs(lasso_probs[,2] - lasso_probs[,3])
accuracy <- 1 - sum(lasso_probs$result)/length(ytest)
accuracy
train = filter(full_matrix, game_year %in% c(2008, 2009, 2010, 2011, 2012))
test = filter(full_matrix, game_year == 2013)
xtest = as.matrix(test[,8:44])
ytest = as.matrix(test[,2])
xtrain = as.matrix(train[,8:44])
ytrain = as.matrix(train[,2])
# Fit a LARS object
lasso_mod <- lars(xtrain, ytrain, type="lasso")
fits <- predict.lars(lasso_mod, xtest, type="fit")
p <- as.data.frame(fits[4])
lasso_probs <- as.data.frame(p[,56])
lasso_probs$class <- ifelse(lasso_probs[,1] >= .5, 1, 0)
lasso_probs <- cbind(lasso_probs, ytest)
lasso_probs$result <- abs(lasso_probs[,2] - lasso_probs[,3])
accuracy <- 1 - sum(lasso_probs$result)/length(ytest)
accuracy
train = filter(full_matrix, game_year %in% c(2008))
test = filter(full_matrix, game_year == 2009)
xtest = as.matrix(test[,8:44])
ytest = as.matrix(test[,2])
xtrain = as.matrix(train[,8:44])
ytrain = as.matrix(train[,2])
# Fit a LARS object
lasso_mod <- lars(xtrain, ytrain, type="lasso")
fits <- predict.lars(lasso_mod, xtest, type="fit")
p <- as.data.frame(fits[4])
lasso_probs <- as.data.frame(p[,56])
lasso_probs$class <- ifelse(lasso_probs[,1] >= .5, 1, 0)
lasso_probs <- cbind(lasso_probs, ytest)
lasso_probs$result <- abs(lasso_probs[,2] - lasso_probs[,3])
accuracy <- 1 - sum(lasso_probs$result)/length(ytest)
accuracy
train = filter(full_matrix, game_year %in% c(2008, 2009))
test = filter(full_matrix, game_year == 2010)
xtest = as.matrix(test[,8:44])
ytest = as.matrix(test[,2])
xtrain = as.matrix(train[,8:44])
ytrain = as.matrix(train[,2])
# Fit a LARS object
lasso_mod <- lars(xtrain, ytrain, type="lasso")
fits <- predict.lars(lasso_mod, xtest, type="fit")
p <- as.data.frame(fits[4])
lasso_probs <- as.data.frame(p[,56])
lasso_probs$class <- ifelse(lasso_probs[,1] >= .5, 1, 0)
lasso_probs <- cbind(lasso_probs, ytest)
lasso_probs$result <- abs(lasso_probs[,2] - lasso_probs[,3])
accuracy <- 1 - sum(lasso_probs$result)/length(ytest)
accuracy
train = filter(full_matrix, game_year %in% c(2008, 2009, 2010))
test = filter(full_matrix, game_year == 2011)
xtest = as.matrix(test[,8:44])
ytest = as.matrix(test[,2])
xtrain = as.matrix(train[,8:44])
ytrain = as.matrix(train[,2])
# Fit a LARS object
lasso_mod <- lars(xtrain, ytrain, type="lasso")
fits <- predict.lars(lasso_mod, xtest, type="fit")
p <- as.data.frame(fits[4])
lasso_probs <- as.data.frame(p[,56])
lasso_probs$class <- ifelse(lasso_probs[,1] >= .5, 1, 0)
lasso_probs <- cbind(lasso_probs, ytest)
lasso_probs$result <- abs(lasso_probs[,2] - lasso_probs[,3])
accuracy <- 1 - sum(lasso_probs$result)/length(ytest)
accuracy
train = filter(full_matrix, game_year %in% c(2008, 2009, 2010, 2011))
test = filter(full_matrix, game_year == 2012)
xtest = as.matrix(test[,8:44])
ytest = as.matrix(test[,2])
xtrain = as.matrix(train[,8:44])
ytrain = as.matrix(train[,2])
# Fit a LARS object
lasso_mod <- lars(xtrain, ytrain, type="lasso")
fits <- predict.lars(lasso_mod, xtest, type="fit")
p <- as.data.frame(fits[4])
lasso_probs <- as.data.frame(p[,56])
lasso_probs$class <- ifelse(lasso_probs[,1] >= .5, 1, 0)
lasso_probs <- cbind(lasso_probs, ytest)
lasso_probs$result <- abs(lasso_probs[,2] - lasso_probs[,3])
accuracy <- 1 - sum(lasso_probs$result)/length(ytest)
accuracy
## Lasso
train = filter(full_matrix, game_year %in% c(2008, 2009, 2010, 2011, 2012))
test = filter(full_matrix, game_year == 2013)
xtest = as.matrix(test[,8:44])
ytest = as.matrix(test[,2])
xtrain = as.matrix(train[,8:44])
ytrain = as.matrix(train[,2])
# Fit a LARS object
lasso_mod <- lars(xtrain, ytrain, type="lasso")
fits <- predict.lars(lasso_mod, xtest, type="fit")
p <- as.data.frame(fits[4])
lasso_probs <- as.data.frame(p[,56])
lasso_probs$class <- ifelse(lasso_probs[,1] >= .5, 1, 0)
lasso_probs <- cbind(lasso_probs, ytest)
lasso_probs$result <- abs(lasso_probs[,2] - lasso_probs[,3])
accuracy <- 1 - sum(lasso_probs$result)/length(ytest)
accuracy
